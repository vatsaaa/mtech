{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e826906",
   "metadata": {},
   "source": [
    "### The Smart Supplier: Optimizing Orders in a Fluctuating Market - 6 Marks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2beffcce",
   "metadata": {},
   "source": [
    "Develop a reinforcement learning agent using dynamic programming to help a Smart Supplier decide which products to manufacture and sell each day to maximize profit. The agent must learn the optimal policy for choosing daily production quantities, considering its limited raw materials and the unpredictable daily demand and selling prices for different products.\n",
    "\n",
    "#### **Scenario**\n",
    " A small Smart Supplier manufactures two simple products: Product A and Product B. Each day, the supplier has a limited amount of raw material. The challenge is that the market demand and selling price for Product A and Product B change randomly each day, making some products more profitable than others at different times. The supplier needs to decide how much of each product to produce to maximize profit while managing their limited raw material.\n",
    "\n",
    "#### **Objective**\n",
    "The Smart Supplier's agent must learn the optimal policy Ï€âˆ— using dynamic programming (Value Iteration or Policy Iteration) to decide how many units of Product A and Product B to produce each day to maximize the total profit over the fixed number of days, given the daily changing market conditions and limited raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f4feac",
   "metadata": {},
   "source": [
    "### --- 1. Custom Environment Creation (SmartSupplierEnv) --- ( 1 Mark )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b172f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a4a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define market states and their product prices\n",
    "class MarketState(Enum):\n",
    "    \"\"\"\n",
    "    MARKET STATE ENUMERATION\n",
    "    =======================\n",
    "    \n",
    "    Represents the two possible market conditions that affect product prices:\n",
    "    \n",
    "    HIGH_DEMAND_A (State 1): Market favors Product A\n",
    "    - Product A: $8 per unit (high profit)\n",
    "    - Product B: $2 per unit (low profit)\n",
    "    \n",
    "    HIGH_DEMAND_B (State 2): Market favors Product B  \n",
    "    - Product A: $3 per unit (low profit)\n",
    "    - Product B: $5 per unit (high profit)\n",
    "    \n",
    "    This enum provides type safety and clear naming for market conditions.\n",
    "    \"\"\"\n",
    "    HIGH_DEMAND_A = 1\n",
    "    HIGH_DEMAND_B = 2\n",
    "\n",
    "class Action(Enum):\n",
    "    \"\"\"\n",
    "    ACTION SPACE ENUMERATION\n",
    "    =======================\n",
    "    \n",
    "    Defines all possible production decisions the supplier can make.\n",
    "    Each action specifies how many units of Product A and B to produce.\n",
    "    \n",
    "    FORMAT: PRODUCE_XA_YB means produce X units of A and Y units of B\n",
    "    \n",
    "    RESOURCE REQUIREMENTS:\n",
    "    - Product A costs 2 raw materials per unit\n",
    "    - Product B costs 1 raw material per unit\n",
    "    - Total raw materials available: 10 units per day\n",
    "    \n",
    "    ACTION VALIDATION:\n",
    "    - PRODUCE_2A_0B: 2*2 + 0*1 = 4 RM (valid)\n",
    "    - PRODUCE_1A_2B: 1*2 + 2*1 = 4 RM (valid)\n",
    "    - PRODUCE_0A_5B: 0*2 + 5*1 = 5 RM (valid)\n",
    "    - PRODUCE_3A_0B: 3*2 + 0*1 = 6 RM (valid)\n",
    "    - DO_NOTHING: 0*2 + 0*1 = 0 RM (valid)\n",
    "    \"\"\"\n",
    "    PRODUCE_2A_0B = (2, 0)  # 2 units A, 0 units B (4 RM)\n",
    "    PRODUCE_1A_2B = (1, 2)  # 1 unit A, 2 units B (4 RM)\n",
    "    PRODUCE_0A_5B = (0, 5)  # 0 units A, 5 units B (5 RM)\n",
    "    PRODUCE_3A_0B = (3, 0)  # 3 units A, 0 units B (6 RM)\n",
    "    DO_NOTHING = (0, 0)     # 0 units A, 0 units B (0 RM)\n",
    "\n",
    "# Structure: {Market_State_ID: {'A_price': X, 'B_price': Y}}\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    \"\"\"\n",
    "    STATE REPRESENTATION\n",
    "    ===================\n",
    "    \n",
    "    Complete description of the system state at any point in time.\n",
    "    Uses @dataclass for automatic __init__, __repr__, and __hash__ methods.\n",
    "    frozen=True makes instances immutable (required for dictionary keys).\n",
    "    \n",
    "    STATE COMPONENTS:\n",
    "    ----------------\n",
    "    day: Current day (1-5)\n",
    "         - Important for finite horizon planning\n",
    "         - Affects remaining opportunities for profit\n",
    "         \n",
    "    raw_material: Available raw materials (0-10)\n",
    "                 - Constrains available actions\n",
    "                 - Resets to 10 at start of each day\n",
    "                 \n",
    "    market_state: Current market condition (MarketState enum)\n",
    "                 - Determines product prices\n",
    "                 - Changes randomly each day\n",
    "    \n",
    "    STATE SPACE SIZE:\n",
    "    ----------------\n",
    "    Total states = Days Ã— Raw Materials Ã— Market States\n",
    "                 = 5 Ã— 11 Ã— 2 = 110 states\n",
    "    \n",
    "    This manageable state space allows exact dynamic programming solutions.\n",
    "    \"\"\"\n",
    "    day: int              # Current day (1-5)\n",
    "    raw_material: int     # Available raw materials (0-10)\n",
    "    market_state: MarketState  # Current market condition\n",
    "\n",
    "# Define product raw material costs\n",
    "\n",
    "# Define actions: (num_A, num_B, raw_material_cost_precalculated)\n",
    "        # Action ID mapping:\n",
    "        # 0: Produce_2A_0B\n",
    "        # 1: Produce_1A_2B\n",
    "        # 2: Produce_0A_5B\n",
    "        # 3: Produce_3A_0B\n",
    "        # 4: Do_Nothing\n",
    "\n",
    " # Define state space dimensions\n",
    "        # Current Day: 1 to num_days\n",
    "        # Current Raw Material: 0 to initial_raw_material\n",
    "        # Current Market State: 1 or 2\n",
    "\n",
    "# get reward function\n",
    "\n",
    "class SmartSupplierEnvironment:\n",
    "    \"\"\"\n",
    "    SMART SUPPLIER ENVIRONMENT IMPLEMENTATION\n",
    "    ========================================\n",
    "    \n",
    "    This class implements the complete environment dynamics for the Smart Supplier\n",
    "    problem, including state transitions, reward calculations, and constraint checking.\n",
    "    \n",
    "    ENVIRONMENT CHARACTERISTICS:\n",
    "    ---------------------------\n",
    "    - FINITE HORIZON: Episodes last exactly 5 days\n",
    "    - STOCHASTIC TRANSITIONS: Market state changes randomly\n",
    "    - DETERMINISTIC REWARDS: Given state and action, reward is deterministic  \n",
    "    - RESOURCE CONSTRAINTS: Limited raw materials constrain feasible actions\n",
    "    - DAILY RESET: Raw materials reset to 10 each day\n",
    "    \n",
    "    TRANSITION DYNAMICS:\n",
    "    -------------------\n",
    "    When an action is taken in state (day, rm, market):\n",
    "    1. Check if action is feasible given raw materials\n",
    "    2. Calculate immediate reward from production\n",
    "    3. Advance to next day with reset raw materials\n",
    "    4. Market state transitions with 50% probability each\n",
    "    \n",
    "    REWARD STRUCTURE:\n",
    "    ----------------\n",
    "    Rewards are calculated as: (units_A * price_A) + (units_B * price_B)\n",
    "    Prices depend on current market state:\n",
    "    - Market State 1: A=$8, B=$2\n",
    "    - Market State 2: A=$3, B=$5\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        ENVIRONMENT INITIALIZATION\n",
    "        =========================\n",
    "        \n",
    "        Set up all environment parameters including:\n",
    "        - Product costs and prices\n",
    "        - Raw material limits\n",
    "        - Episode duration\n",
    "        - Market transition probabilities\n",
    "        \"\"\"\n",
    "        # PRODUCTION COSTS (in raw materials)\n",
    "        self.cost_A = 2  # Product A costs 2 RM per unit\n",
    "        self.cost_B = 1  # Product B costs 1 RM per unit\n",
    "        \n",
    "        # DAILY RESOURCE ALLOCATION\n",
    "        self.initial_raw_material = 10  # Start each day with 10 RM\n",
    "        \n",
    "        # EPISODE CONFIGURATION\n",
    "        self.max_days = 5  # Episode lasts 5 days\n",
    "        \n",
    "        # MARKET PRICING STRUCTURE\n",
    "        # Dictionary mapping market state to (price_A, price_B)\n",
    "        self.market_prices = {\n",
    "            MarketState.HIGH_DEMAND_A: (8, 2),  # A favored: A=$8, B=$2\n",
    "            MarketState.HIGH_DEMAND_B: (3, 5)   # B favored: A=$3, B=$5\n",
    "        }\n",
    "        \n",
    "        # MARKET TRANSITION PROBABILITIES\n",
    "        # 50% chance of each market state each day (independent)\n",
    "        self.market_transition_prob = 0.5\n",
    "        \n",
    "        print(\"SMART SUPPLIER ENVIRONMENT INITIALIZED\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Episode length: {self.max_days} days\")\n",
    "        print(f\"Daily raw materials: {self.initial_raw_material} units\")\n",
    "        print(f\"Product A cost: {self.cost_A} RM per unit\")\n",
    "        print(f\"Product B cost: {self.cost_B} RM per unit\")\n",
    "        print(\"\\nMarket Pricing:\")\n",
    "        for market, (price_a, price_b) in self.market_prices.items():\n",
    "            print(f\"  {market.name}: A=${price_a}, B=${price_b}\")\n",
    "    \n",
    "    def is_action_feasible(self, state: State, action: Action) -> bool:\n",
    "        \"\"\"\n",
    "        ACTION FEASIBILITY CHECK\n",
    "        =======================\n",
    "        \n",
    "        Determines whether a given action can be executed in the current state.\n",
    "        An action is feasible if the required raw materials don't exceed available resources.\n",
    "        \n",
    "        FEASIBILITY CONSTRAINT:\n",
    "        ----------------------\n",
    "        total_cost = (units_A * cost_A) + (units_B * cost_B) â‰¤ available_RM\n",
    "        \n",
    "        INFEASIBLE ACTION HANDLING:\n",
    "        --------------------------\n",
    "        If an action is infeasible, it results in no production and zero reward.\n",
    "        This models the real-world constraint that you can't produce more than\n",
    "        your resources allow.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state of the environment\n",
    "            action: Action to check for feasibility\n",
    "            \n",
    "        Returns:\n",
    "            bool: True if action can be executed, False otherwise\n",
    "        \"\"\"\n",
    "        units_A, units_B = action.value\n",
    "        total_cost = (units_A * self.cost_A) + (units_B * self.cost_B)\n",
    "        return total_cost <= state.raw_material\n",
    "    \n",
    "    def get_reward(self, state: State, action: Action) -> float:\n",
    "        \"\"\"\n",
    "        REWARD CALCULATION\n",
    "        ==================\n",
    "        \n",
    "        Calculates the immediate reward for taking a specific action in a given state.\n",
    "        Reward represents the profit from selling produced units at current market prices.\n",
    "        \n",
    "        REWARD FORMULA:\n",
    "        --------------\n",
    "        If action is feasible:\n",
    "            reward = (units_A * current_price_A) + (units_B * current_price_B)\n",
    "        If action is infeasible:\n",
    "            reward = 0 (no production occurs)\n",
    "        \n",
    "        MARKET PRICE DEPENDENCY:\n",
    "        -----------------------\n",
    "        Prices depend on current market state:\n",
    "        - HIGH_DEMAND_A: Favors Product A (A=$8, B=$2)\n",
    "        - HIGH_DEMAND_B: Favors Product B (A=$3, B=$5)\n",
    "        \n",
    "        ECONOMIC INTERPRETATION:\n",
    "        -----------------------\n",
    "        This reward structure captures the core economic trade-off:\n",
    "        - Different products have different profitability in different markets\n",
    "        - Resource constraints limit production possibilities\n",
    "        - Optimal decisions must consider both current prices and resource costs\n",
    "        \n",
    "        Args:\n",
    "            state: Current state (includes market condition)\n",
    "            action: Production decision to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            float: Immediate profit from the action\n",
    "        \"\"\"\n",
    "        # Check if action is feasible given resource constraints\n",
    "        if not self.is_action_feasible(state, action):\n",
    "            return 0.0  # No production, no profit\n",
    "        \n",
    "        # Extract production quantities from action\n",
    "        units_A, units_B = action.value\n",
    "        \n",
    "        # Get current market prices\n",
    "        price_A, price_B = self.market_prices[state.market_state]\n",
    "        \n",
    "        # Calculate total revenue (profit)\n",
    "        revenue = (units_A * price_A) + (units_B * price_B)\n",
    "        \n",
    "        return float(revenue)\n",
    "    \n",
    "    def get_next_states(self, state: State, action: Action) -> List[Tuple[State, float]]:\n",
    "        \"\"\"\n",
    "        STATE TRANSITION DYNAMICS\n",
    "        ========================\n",
    "        \n",
    "        Returns all possible next states and their probabilities after taking an action.\n",
    "        This is crucial for dynamic programming algorithms that need to consider all\n",
    "        possible outcomes when computing expected values.\n",
    "        \n",
    "        TRANSITION MECHANICS:\n",
    "        --------------------\n",
    "        1. Day advances by 1 (deterministic)\n",
    "        2. Raw materials reset to 10 (deterministic)\n",
    "        3. Market state changes (stochastic)\n",
    "        \n",
    "        MARKET STATE TRANSITIONS:\n",
    "        ------------------------\n",
    "        Each day, market state is independently determined:\n",
    "        - P(HIGH_DEMAND_A) = 0.5\n",
    "        - P(HIGH_DEMAND_B) = 0.5\n",
    "        \n",
    "        This creates a Markov process where future market conditions don't depend\n",
    "        on past market conditions (realistic assumption for daily price fluctuations).\n",
    "        \n",
    "        EPISODE TERMINATION:\n",
    "        -------------------\n",
    "        If we're on the last day (day 5), episode terminates and no next states exist.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action taken (doesn't affect transitions in this environment)\n",
    "            \n",
    "        Returns:\n",
    "            List of (next_state, probability) tuples\n",
    "        \"\"\"\n",
    "        # Check for episode termination\n",
    "        if state.day >= self.max_days:\n",
    "            return []  # No next states - episode ends\n",
    "        \n",
    "        # Calculate next day\n",
    "        next_day = state.day + 1\n",
    "        \n",
    "        # Raw materials reset to full amount each day\n",
    "        next_raw_material = self.initial_raw_material\n",
    "        \n",
    "        # Generate all possible next states (one for each market condition)\n",
    "        next_states = []\n",
    "        \n",
    "        for market_state in MarketState:\n",
    "            next_state = State(\n",
    "                day=next_day,\n",
    "                raw_material=next_raw_material,\n",
    "                market_state=market_state\n",
    "            )\n",
    "            # Each market state has equal probability (50%)\n",
    "            probability = self.market_transition_prob\n",
    "            next_states.append((next_state, probability))\n",
    "        \n",
    "        return next_states\n",
    "    \n",
    "    def get_all_states(self) -> List[State]:\n",
    "        \"\"\"\n",
    "        COMPLETE STATE SPACE ENUMERATION\n",
    "        ===============================\n",
    "        \n",
    "        Generates all possible states in the environment for exact dynamic programming.\n",
    "        This is feasible because our state space is relatively small (110 states).\n",
    "        \n",
    "        STATE SPACE STRUCTURE:\n",
    "        ---------------------\n",
    "        - Days: 1, 2, 3, 4, 5 (5 values)\n",
    "        - Raw Materials: 0, 1, 2, ..., 10 (11 values)\n",
    "        - Market States: HIGH_DEMAND_A, HIGH_DEMAND_B (2 values)\n",
    "        \n",
    "        Total: 5 Ã— 11 Ã— 2 = 110 states\n",
    "        \n",
    "        DYNAMIC PROGRAMMING REQUIREMENT:\n",
    "        -------------------------------\n",
    "        Exact DP algorithms require knowing all states to:\n",
    "        1. Initialize value function for all states\n",
    "        2. Perform value updates across all states\n",
    "        3. Extract optimal policy for all states\n",
    "        \n",
    "        Returns:\n",
    "            List of all possible State objects\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        \n",
    "        # Enumerate all combinations of state variables\n",
    "        for day in range(1, self.max_days + 1):\n",
    "            for raw_material in range(self.initial_raw_material + 1):\n",
    "                for market_state in MarketState:\n",
    "                    state = State(\n",
    "                        day=day,\n",
    "                        raw_material=raw_material,\n",
    "                        market_state=market_state\n",
    "                    )\n",
    "                    states.append(state)\n",
    "        \n",
    "        return states\n",
    "    \n",
    "    def get_feasible_actions(self, state: State) -> List[Action]:\n",
    "        \"\"\"\n",
    "        FEASIBLE ACTION IDENTIFICATION\n",
    "        =============================\n",
    "        \n",
    "        Returns all actions that can be legally executed in the given state.\n",
    "        This is essential for policy optimization - we only consider actions\n",
    "        that don't violate resource constraints.\n",
    "        \n",
    "        CONSTRAINT CHECKING:\n",
    "        -------------------\n",
    "        For each action, verify that:\n",
    "        required_RM = (units_A Ã— cost_A) + (units_B Ã— cost_B) â‰¤ available_RM\n",
    "        \n",
    "        EMPTY ACTION SET HANDLING:\n",
    "        -------------------------\n",
    "        In extreme cases (very low raw materials), only DO_NOTHING might be feasible.\n",
    "        This ensures the agent always has at least one legal action.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state to check actions against\n",
    "            \n",
    "        Returns:\n",
    "            List of feasible Action enums\n",
    "        \"\"\"\n",
    "        feasible_actions = []\n",
    "        \n",
    "        for action in Action:\n",
    "            if self.is_action_feasible(state, action):\n",
    "                feasible_actions.append(action)\n",
    "        \n",
    "        return feasible_actions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89b2487",
   "metadata": {},
   "source": [
    "### --- 2. Dynamic Programming Implementation (Value Iteration or Policy Iteration) --- (2 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "027db857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Value Iteration function\n",
    "class ValueIteration:\n",
    "    \"\"\"\n",
    "    VALUE ITERATION ALGORITHM IMPLEMENTATION\n",
    "    =======================================\n",
    "    \n",
    "    Value Iteration is a dynamic programming algorithm that computes the optimal\n",
    "    value function V*(s) and optimal policy Ï€*(s) for Markov Decision Processes.\n",
    "    \n",
    "    ALGORITHM OVERVIEW:\n",
    "    ==================\n",
    "    Value Iteration works by iteratively updating value estimates using the\n",
    "    Bellman optimality equation until convergence:\n",
    "    \n",
    "    V_{k+1}(s) = max_a Î£_{s'} P(s'|s,a) Ã— [R(s,a,s') + Î³ Ã— V_k(s')]\n",
    "    \n",
    "    ALGORITHM STEPS:\n",
    "    ===============\n",
    "    1. Initialize V(s) = 0 for all states s\n",
    "    2. Repeat until convergence:\n",
    "       a. For each state s:\n",
    "          - For each action a:\n",
    "            - Calculate expected value: Q(s,a) = Î£ P(s'|s,a)[R(s,a,s') + Î³V(s')]\n",
    "          - Update V(s) = max_a Q(s,a)\n",
    "    3. Extract optimal policy: Ï€*(s) = argmax_a Q(s,a)\n",
    "    \n",
    "    CONVERGENCE CRITERIA:\n",
    "    ====================\n",
    "    The algorithm stops when the maximum change in value function across all\n",
    "    states falls below a threshold (typically 1e-6).\n",
    "    \n",
    "    FINITE HORIZON CONSIDERATIONS:\n",
    "    =============================\n",
    "    For our 5-day problem, we use Î³=1.0 (no discounting) since:\n",
    "    - Episode has definite end point\n",
    "    - All rewards are equally important regardless of timing\n",
    "    - Terminal states (day > 5) have V = 0\n",
    "    \n",
    "    COMPUTATIONAL COMPLEXITY:\n",
    "    ========================\n",
    "    - Time: O(iterations Ã— |S| Ã— |A| Ã— |S|)\n",
    "    - Space: O(|S|)\n",
    "    - For our problem: ~O(iterations Ã— 110 Ã— 5 Ã— 110) operations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, environment: SmartSupplierEnvironment, gamma: float = 1.0, \n",
    "                 tolerance: float = 1e-6, max_iterations: int = 1000):\n",
    "        \"\"\"\n",
    "        VALUE ITERATION INITIALIZATION\n",
    "        =============================\n",
    "        \n",
    "        Args:\n",
    "            environment: The Smart Supplier environment\n",
    "            gamma: Discount factor (1.0 for finite horizon problems)\n",
    "            tolerance: Convergence threshold for value function changes\n",
    "            max_iterations: Maximum iterations to prevent infinite loops\n",
    "        \"\"\"\n",
    "        self.env = environment\n",
    "        self.gamma = gamma\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iterations = max_iterations\n",
    "        \n",
    "        # Initialize data structures\n",
    "        self.states = self.env.get_all_states()\n",
    "        self.value_function = {}  # V(s) for each state\n",
    "        self.policy = {}          # Ï€(s) for each state\n",
    "        self.q_values = {}        # Q(s,a) for state-action pairs\n",
    "        \n",
    "        # Performance tracking\n",
    "        self.iteration_count = 0\n",
    "        self.convergence_history = []\n",
    "        \n",
    "        print(\"VALUE ITERATION ALGORITHM INITIALIZED\")\n",
    "        print(\"=\" * 40)\n",
    "        print(f\"State space size: {len(self.states)}\")\n",
    "        print(f\"Discount factor: {self.gamma}\")\n",
    "        print(f\"Convergence tolerance: {self.tolerance}\")\n",
    "        print(f\"Maximum iterations: {self.max_iterations}\")\n",
    "    \n",
    "    def initialize_value_function(self):\n",
    "        \"\"\"\n",
    "        VALUE FUNCTION INITIALIZATION\n",
    "        ============================\n",
    "        \n",
    "        Initialize the value function for all states. For finite horizon problems,\n",
    "        we typically start with V(s) = 0 for all states.\n",
    "        \n",
    "        TERMINAL STATE HANDLING:\n",
    "        -----------------------\n",
    "        States beyond the episode horizon (day > max_days) have V = 0 permanently,\n",
    "        representing that no further rewards can be obtained.\n",
    "        \n",
    "        INITIAL VALUE CHOICE:\n",
    "        --------------------\n",
    "        V(s) = 0 is a common choice because:\n",
    "        - Conservative estimate (underestimates true values initially)\n",
    "        - Mathematically sound (values will increase toward optimal)\n",
    "        - Simple and interpretable\n",
    "        \"\"\"\n",
    "        print(\"\\nInitializing value function...\")\n",
    "        \n",
    "        for state in self.states:\n",
    "            # All states start with zero value\n",
    "            self.value_function[state] = 0.0\n",
    "        \n",
    "        print(f\"Initialized {len(self.value_function)} states with V(s) = 0\")\n",
    "    \n",
    "    def compute_q_value(self, state: State, action: Action) -> float:\n",
    "        \"\"\"\n",
    "        Q-VALUE COMPUTATION (ACTION-VALUE FUNCTION)\n",
    "        ==========================================\n",
    "        \n",
    "        Computes Q(s,a) = expected total reward from taking action a in state s\n",
    "        and then following the optimal policy thereafter.\n",
    "        \n",
    "        Q-VALUE FORMULA:\n",
    "        ---------------\n",
    "        Q(s,a) = R(s,a) + Î³ Ã— Î£_{s'} P(s'|s,a) Ã— V(s')\n",
    "        \n",
    "        Where:\n",
    "        - R(s,a): Immediate reward from taking action a in state s\n",
    "        - Î³: Discount factor\n",
    "        - P(s'|s,a): Probability of transitioning to state s'\n",
    "        - V(s'): Current value estimate for next state s'\n",
    "        \n",
    "        EXPECTATION CALCULATION:\n",
    "        -----------------------\n",
    "        Since our environment is stochastic (random market transitions), we must\n",
    "        compute the expected value over all possible next states weighted by\n",
    "        their transition probabilities.\n",
    "        \n",
    "        Args:\n",
    "            state: Current state\n",
    "            action: Action to evaluate\n",
    "            \n",
    "        Returns:\n",
    "            float: Q-value for the state-action pair\n",
    "        \"\"\"\n",
    "        # Get immediate reward\n",
    "        immediate_reward = self.env.get_reward(state, action)\n",
    "        \n",
    "        # Get all possible next states and their probabilities\n",
    "        next_states = self.env.get_next_states(state, action)\n",
    "        \n",
    "        # Compute expected future value\n",
    "        expected_future_value = 0.0\n",
    "        for next_state, probability in next_states:\n",
    "            expected_future_value += probability * self.value_function[next_state]\n",
    "        \n",
    "        # Q-value = immediate reward + discounted expected future value\n",
    "        q_value = immediate_reward + self.gamma * expected_future_value\n",
    "        \n",
    "        return q_value\n",
    "    \n",
    "    def value_iteration_step(self) -> float:\n",
    "        \"\"\"\n",
    "        SINGLE VALUE ITERATION UPDATE\n",
    "        ============================\n",
    "        \n",
    "        Performs one complete sweep through all states, updating their values\n",
    "        according to the Bellman optimality equation.\n",
    "        \n",
    "        BELLMAN OPTIMALITY UPDATE:\n",
    "        --------------------------\n",
    "        For each state s:\n",
    "        V_{new}(s) = max_a Q(s,a)\n",
    "        \n",
    "        This represents the maximum expected total reward achievable from state s.\n",
    "        \n",
    "        CONVERGENCE MEASUREMENT:\n",
    "        -----------------------\n",
    "        We track the maximum absolute change in value function:\n",
    "        max_change = max_s |V_{new}(s) - V_{old}(s)|\n",
    "        \n",
    "        When max_change < tolerance, the algorithm has converged.\n",
    "        \n",
    "        SYNCHRONOUS UPDATES:\n",
    "        -------------------\n",
    "        We compute all new values before updating any state's value.\n",
    "        This ensures consistent computation across the entire state space.\n",
    "        \n",
    "        Returns:\n",
    "            float: Maximum absolute change in value function\n",
    "        \"\"\"\n",
    "        new_values = {}\n",
    "        max_change = 0.0\n",
    "        \n",
    "        # Compute new values for all states\n",
    "        for state in self.states:\n",
    "            # Get all feasible actions for this state\n",
    "            feasible_actions = self.env.get_feasible_actions(state)\n",
    "            \n",
    "            if not feasible_actions:\n",
    "                # Edge case: no feasible actions (shouldn't happen in our environment)\n",
    "                new_values[state] = 0.0\n",
    "                continue\n",
    "            \n",
    "            # Compute Q-values for all feasible actions\n",
    "            q_values = []\n",
    "            for action in feasible_actions:\n",
    "                q_val = self.compute_q_value(state, action)\n",
    "                q_values.append(q_val)\n",
    "            \n",
    "            # Bellman optimality: V(s) = max_a Q(s,a)\n",
    "            new_values[state] = max(q_values)\n",
    "            \n",
    "            # Track convergence\n",
    "            change = abs(new_values[state] - self.value_function[state])\n",
    "            max_change = max(max_change, change)\n",
    "        \n",
    "        # Update value function (synchronous update)\n",
    "        self.value_function = new_values\n",
    "        \n",
    "        return max_change\n",
    "    \n",
    "    def extract_policy(self):\n",
    "        \"\"\"\n",
    "        OPTIMAL POLICY EXTRACTION\n",
    "        ========================\n",
    "        \n",
    "        After value iteration converges, extract the optimal policy by selecting\n",
    "        the action that maximizes Q-value in each state.\n",
    "        \n",
    "        POLICY EXTRACTION FORMULA:\n",
    "        -------------------------\n",
    "        Ï€*(s) = argmax_a Q(s,a)\n",
    "        \n",
    "        This gives us the optimal action to take in each state to maximize\n",
    "        expected total reward.\n",
    "        \n",
    "        Q-VALUE STORAGE:\n",
    "        ---------------\n",
    "        We also store Q-values for analysis and debugging purposes.\n",
    "        Q-values provide insight into the relative value of different actions.\n",
    "        \"\"\"\n",
    "        print(\"\\nExtracting optimal policy...\")\n",
    "        \n",
    "        for state in self.states:\n",
    "            feasible_actions = self.env.get_feasible_actions(state)\n",
    "            \n",
    "            if not feasible_actions:\n",
    "                # Edge case handling\n",
    "                self.policy[state] = Action.DO_NOTHING\n",
    "                continue\n",
    "            \n",
    "            # Compute Q-values for all feasible actions\n",
    "            best_action = None\n",
    "            best_q_value = float('-inf')\n",
    "            state_q_values = {}\n",
    "            \n",
    "            for action in feasible_actions:\n",
    "                q_val = self.compute_q_value(state, action)\n",
    "                state_q_values[action] = q_val\n",
    "                \n",
    "                if q_val > best_q_value:\n",
    "                    best_q_value = q_val\n",
    "                    best_action = action\n",
    "            \n",
    "            # Store optimal action and Q-values\n",
    "            self.policy[state] = best_action\n",
    "            self.q_values[state] = state_q_values\n",
    "        \n",
    "        print(f\"Extracted policy for {len(self.policy)} states\")\n",
    "    \n",
    "    def solve(self) -> Tuple[Dict[State, float], Dict[State, Action]]:\n",
    "        \"\"\"\n",
    "        COMPLETE VALUE ITERATION ALGORITHM\n",
    "        =================================\n",
    "        \n",
    "        Runs the full value iteration algorithm until convergence, then extracts\n",
    "        the optimal policy.\n",
    "        \n",
    "        ALGORITHM EXECUTION:\n",
    "        -------------------\n",
    "        1. Initialize value function\n",
    "        2. Iterate until convergence:\n",
    "           - Update all state values using Bellman equation\n",
    "           - Check for convergence\n",
    "        3. Extract optimal policy from converged value function\n",
    "        \n",
    "        CONVERGENCE MONITORING:\n",
    "        ----------------------\n",
    "        We track convergence history to analyze algorithm behavior and\n",
    "        ensure proper convergence.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (value_function, optimal_policy)\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING VALUE ITERATION ALGORITHM\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 1: Initialize\n",
    "        self.initialize_value_function()\n",
    "        \n",
    "        # Step 2: Iterate until convergence\n",
    "        print(\"\\nIterating toward optimal value function...\")\n",
    "        for iteration in range(self.max_iterations):\n",
    "            max_change = self.value_iteration_step()\n",
    "            self.convergence_history.append(max_change)\n",
    "            self.iteration_count = iteration + 1\n",
    "            \n",
    "            # Progress reporting\n",
    "            if iteration % 10 == 0 or max_change < self.tolerance:\n",
    "                print(f\"  Iteration {iteration+1:3d}: Max change = {max_change:.8f}\")\n",
    "            \n",
    "            # Check for convergence\n",
    "            if max_change < self.tolerance:\n",
    "                print(f\"\\nâœ“ CONVERGED after {iteration+1} iterations!\")\n",
    "                print(f\"  Final max change: {max_change:.2e}\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"\\nâš  Reached maximum iterations ({self.max_iterations})\")\n",
    "            print(f\"  Final max change: {max_change:.2e}\")\n",
    "        \n",
    "        # Step 3: Extract optimal policy\n",
    "        self.extract_policy()\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ VALUE ITERATION COMPLETE\")\n",
    "        print(f\"   Convergence achieved in {self.iteration_count} iterations\")\n",
    "        \n",
    "        return self.value_function, self.policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39cfa71",
   "metadata": {},
   "source": [
    "#### --- 3. Simulation and Policy Analysis ---  ( 1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62490896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate policy function - Simulates the learned policy over multiple runs to evaluate performance\n",
    "class PolicySimulator:\n",
    "    \"\"\"\n",
    "    POLICY SIMULATION AND PERFORMANCE EVALUATION\n",
    "    ===========================================\n",
    "    \n",
    "    This class simulates the learned optimal policy over multiple episodes\n",
    "    to evaluate its real-world performance and validate the theoretical\n",
    "    optimal value function.\n",
    "    \n",
    "    SIMULATION METHODOLOGY:\n",
    "    ======================\n",
    "    1. Run multiple independent episodes (e.g., 1000 runs)\n",
    "    2. In each episode, follow the optimal policy for 5 days\n",
    "    3. Record total rewards and decision patterns\n",
    "    4. Compute performance statistics\n",
    "    \n",
    "    VALIDATION CHECKS:\n",
    "    =================\n",
    "    - Average simulated reward should match theoretical value function\n",
    "    - Policy should adapt correctly to market state changes\n",
    "    - Resource constraints should be respected in all decisions\n",
    "    \n",
    "    PERFORMANCE METRICS:\n",
    "    ===================\n",
    "    - Average total profit per episode\n",
    "    - Standard deviation of profits (risk measure)\n",
    "    - Action frequency distribution\n",
    "    - Market adaptation effectiveness\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, environment: SmartSupplierEnvironment, \n",
    "                 policy: Dict[State, Action]):\n",
    "        \"\"\"\n",
    "        Initialize the policy simulator.\n",
    "        \n",
    "        Args:\n",
    "            environment: The Smart Supplier environment\n",
    "            policy: Optimal policy to simulate\n",
    "        \"\"\"\n",
    "        self.env = environment\n",
    "        self.policy = policy\n",
    "        self.simulation_results = []\n",
    "    \n",
    "    def simulate_episode(self) -> Tuple[float, List[Tuple[State, Action, float]]]:\n",
    "        \"\"\"\n",
    "        SINGLE EPISODE SIMULATION\n",
    "        ========================\n",
    "        \n",
    "        Simulates one complete 5-day episode following the optimal policy.\n",
    "        \n",
    "        EPISODE FLOW:\n",
    "        ------------\n",
    "        1. Start on Day 1 with 10 RM and random market state\n",
    "        2. For each day:\n",
    "           a. Observe current state\n",
    "           b. Take action according to optimal policy\n",
    "           c. Receive reward\n",
    "           d. Transition to next day (reset RM, new market state)\n",
    "        3. Episode ends after Day 5\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (total_reward, episode_history)\n",
    "        \"\"\"\n",
    "        # Initialize episode\n",
    "        total_reward = 0.0\n",
    "        episode_history = []\n",
    "        \n",
    "        # Random initial market state\n",
    "        initial_market = random.choice(list(MarketState))\n",
    "        current_state = State(\n",
    "            day=1,\n",
    "            raw_material=self.env.initial_raw_material,\n",
    "            market_state=initial_market\n",
    "        )\n",
    "        \n",
    "        # Simulate 5 days\n",
    "        for day in range(5):\n",
    "            # Get optimal action for current state\n",
    "            action = self.policy[current_state]\n",
    "            \n",
    "            # Calculate reward\n",
    "            reward = self.env.get_reward(current_state, action)\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Record step\n",
    "            episode_history.append((current_state, action, reward))\n",
    "            \n",
    "            # Transition to next day (if not last day)\n",
    "            if day < 4:  # Days 0-3 transition to next day\n",
    "                next_market = random.choice(list(MarketState))\n",
    "                current_state = State(\n",
    "                    day=current_state.day + 1,\n",
    "                    raw_material=self.env.initial_raw_material,\n",
    "                    market_state=next_market\n",
    "                )\n",
    "        \n",
    "        return total_reward, episode_history\n",
    "    \n",
    "    def run_simulation(self, num_episodes: int = 1000) -> Dict:\n",
    "        \"\"\"\n",
    "        COMPREHENSIVE POLICY SIMULATION\n",
    "        ==============================\n",
    "        \n",
    "        Runs multiple episodes to evaluate policy performance statistically.\n",
    "        \n",
    "        STATISTICAL ANALYSIS:\n",
    "        --------------------\n",
    "        - Mean performance: Expected profit under optimal policy\n",
    "        - Variance analysis: Risk and consistency of performance\n",
    "        - Confidence intervals: Statistical reliability bounds\n",
    "        - Action pattern analysis: Behavioral consistency\n",
    "        \n",
    "        Args:\n",
    "            num_episodes: Number of episodes to simulate\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing simulation results and statistics\n",
    "        \"\"\"\n",
    "        print(f\"\\n\" + \"=\"*70)\n",
    "        print(f\"POLICY SIMULATION: {num_episodes} EPISODES\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        self.simulation_results = []\n",
    "        episode_rewards = []\n",
    "        all_actions = []\n",
    "        \n",
    "        # Run simulations\n",
    "        for episode in range(num_episodes):\n",
    "            total_reward, episode_history = self.simulate_episode()\n",
    "            episode_rewards.append(total_reward)\n",
    "            \n",
    "            # Collect actions for analysis\n",
    "            for state, action, reward in episode_history:\n",
    "                all_actions.append(action)\n",
    "            \n",
    "            # Progress reporting\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                current_avg = np.mean(episode_rewards)\n",
    "                print(f\"  Episode {episode+1:4d}: Running average = ${current_avg:.2f}\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        mean_reward = np.mean(episode_rewards)\n",
    "        std_reward = np.std(episode_rewards)\n",
    "        min_reward = np.min(episode_rewards)\n",
    "        max_reward = np.max(episode_rewards)\n",
    "        \n",
    "        # Confidence interval (95%)\n",
    "        confidence_margin = 1.96 * std_reward / np.sqrt(num_episodes)\n",
    "        ci_lower = mean_reward - confidence_margin\n",
    "        ci_upper = mean_reward + confidence_margin\n",
    "        \n",
    "        # Action frequency analysis\n",
    "        action_counts = {action: 0 for action in Action}\n",
    "        for action in all_actions:\n",
    "            action_counts[action] += 1\n",
    "        \n",
    "        total_actions = len(all_actions)\n",
    "        action_percentages = {action: (count/total_actions)*100 \n",
    "                            for action, count in action_counts.items()}\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'num_episodes': num_episodes,\n",
    "            'episode_rewards': episode_rewards,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'min_reward': min_reward,\n",
    "            'max_reward': max_reward,\n",
    "            'confidence_interval': (ci_lower, ci_upper),\n",
    "            'action_counts': action_counts,\n",
    "            'action_percentages': action_percentages\n",
    "        }\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\nðŸ“Š SIMULATION RESULTS SUMMARY:\")\n",
    "        print(f\"   Episodes Simulated: {num_episodes}\")\n",
    "        print(f\"   Average Profit: ${mean_reward:.2f} Â± ${std_reward:.2f}\")\n",
    "        print(f\"   95% Confidence Interval: ${ci_lower:.2f} - ${ci_upper:.2f}\")\n",
    "        print(f\"   Profit Range: ${min_reward:.2f} - ${max_reward:.2f}\")\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ ACTION USAGE DISTRIBUTION:\")\n",
    "        for action, percentage in action_percentages.items():\n",
    "            units_a, units_b = action.value\n",
    "            print(f\"   {action.name:15} | {percentage:5.1f}% | Produces: {units_a}A, {units_b}B\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# analyze policy function - Analyzes and prints snippets of the learned optimal policy\n",
    "class PolicyAnalyzer:\n",
    "    \"\"\"\n",
    "    OPTIMAL POLICY ANALYSIS AND VISUALIZATION\n",
    "    ========================================\n",
    "    \n",
    "    This class provides comprehensive analysis of the learned optimal policy,\n",
    "    examining how decisions change based on different state variables.\n",
    "    \n",
    "    ANALYSIS DIMENSIONS:\n",
    "    ===================\n",
    "    1. MARKET STATE DEPENDENCY: How does policy change with market conditions?\n",
    "    2. RESOURCE CONSTRAINT IMPACT: How do limited raw materials affect decisions?\n",
    "    3. TIME HORIZON EFFECTS: How does proximity to episode end influence strategy?\n",
    "    4. VALUE FUNCTION ANALYSIS: What are the most/least valuable states?\n",
    "    \n",
    "    INSIGHTS PROVIDED:\n",
    "    =================\n",
    "    - Strategic patterns in optimal decision making\n",
    "    - Trade-offs between different products under different conditions\n",
    "    - Resource management strategies\n",
    "    - Time-sensitive decision patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, environment: SmartSupplierEnvironment, \n",
    "                 value_function: Dict[State, float], \n",
    "                 policy: Dict[State, Action]):\n",
    "        \"\"\"\n",
    "        Initialize the policy analyzer.\n",
    "        \n",
    "        Args:\n",
    "            environment: The Smart Supplier environment\n",
    "            value_function: Optimal value function from Value Iteration\n",
    "            policy: Optimal policy from Value Iteration\n",
    "        \"\"\"\n",
    "        self.env = environment\n",
    "        self.value_function = value_function\n",
    "        self.policy = policy\n",
    "    \n",
    "    def analyze_market_dependency(self):\n",
    "        \"\"\"\n",
    "        MARKET STATE DEPENDENCY ANALYSIS\n",
    "        ===============================\n",
    "        \n",
    "        Analyzes how the optimal policy changes based on market conditions.\n",
    "        This reveals whether the agent successfully learns to adapt to\n",
    "        changing market prices.\n",
    "        \n",
    "        KEY QUESTIONS:\n",
    "        -------------\n",
    "        - Does the policy favor Product A when Market State 1 (high A demand)?\n",
    "        - Does the policy favor Product B when Market State 2 (high B demand)?\n",
    "        - Are there states where the policy is invariant to market conditions?\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MARKET STATE DEPENDENCY ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Analyze policy for each market state\n",
    "        for market_state in MarketState:\n",
    "            print(f\"\\nðŸ“Š OPTIMAL STRATEGY IN {market_state.name}:\")\n",
    "            print(f\"   Market Prices: A=${self.env.market_prices[market_state][0]}, \"\n",
    "                  f\"B=${self.env.market_prices[market_state][1]}\")\n",
    "            \n",
    "            # Count action frequencies for this market state\n",
    "            action_counts = {action: 0 for action in Action}\n",
    "            total_states = 0\n",
    "            \n",
    "            for state in self.value_function.keys():\n",
    "                if state.market_state == market_state:\n",
    "                    action = self.policy[state]\n",
    "                    action_counts[action] += 1\n",
    "                    total_states += 1\n",
    "            \n",
    "            # Display action preferences\n",
    "            print(f\"   Action Distribution ({total_states} states):\")\n",
    "            for action, count in action_counts.items():\n",
    "                percentage = (count / total_states) * 100 if total_states > 0 else 0\n",
    "                units_a, units_b = action.value\n",
    "                print(f\"     {action.name:15} | {count:2d} states ({percentage:5.1f}%) | \"\n",
    "                      f\"Produces: {units_a}A, {units_b}B\")\n",
    "    \n",
    "    def analyze_resource_dependency(self):\n",
    "        \"\"\"\n",
    "        RESOURCE CONSTRAINT IMPACT ANALYSIS\n",
    "        ==================================\n",
    "        \n",
    "        Examines how available raw materials influence optimal decisions.\n",
    "        This shows whether the agent learns efficient resource management.\n",
    "        \n",
    "        EXPECTED PATTERNS:\n",
    "        -----------------\n",
    "        - Low RM: Prefer cheaper Product B or do nothing\n",
    "        - High RM: Can afford expensive Product A production\n",
    "        - Medium RM: Balanced production strategies\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"RESOURCE CONSTRAINT IMPACT ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Group states by raw material level\n",
    "        rm_levels = [0, 2, 4, 6, 8, 10]  # Sample key resource levels\n",
    "        \n",
    "        for rm_level in rm_levels:\n",
    "            print(f\"\\nðŸ”‹ STRATEGY WITH {rm_level} RAW MATERIALS:\")\n",
    "            \n",
    "            # Find states with this resource level\n",
    "            relevant_states = [s for s in self.value_function.keys() \n",
    "                             if s.raw_material == rm_level]\n",
    "            \n",
    "            if not relevant_states:\n",
    "                print(\"   No states with this resource level\")\n",
    "                continue\n",
    "            \n",
    "            # Analyze action distribution\n",
    "            action_counts = {action: 0 for action in Action}\n",
    "            for state in relevant_states:\n",
    "                action = self.policy[state]\n",
    "                action_counts[action] += 1\n",
    "            \n",
    "            total_states = len(relevant_states)\n",
    "            print(f\"   Action Distribution ({total_states} states):\")\n",
    "            \n",
    "            for action, count in action_counts.items():\n",
    "                if count > 0:\n",
    "                    percentage = (count / total_states) * 100\n",
    "                    units_a, units_b = action.value\n",
    "                    cost = units_a * 2 + units_b * 1\n",
    "                    feasible = \"âœ“\" if cost <= rm_level else \"âœ—\"\n",
    "                    print(f\"     {action.name:15} | {count:2d} states ({percentage:5.1f}%) | \"\n",
    "                          f\"Cost: {cost}RM {feasible}\")\n",
    "    \n",
    "    def analyze_time_dependency(self):\n",
    "        \"\"\"\n",
    "        TIME HORIZON EFFECTS ANALYSIS\n",
    "        ============================\n",
    "        \n",
    "        Studies how the optimal policy changes as the episode progresses.\n",
    "        This reveals time-sensitive strategic adaptations.\n",
    "        \n",
    "        EXPECTED PATTERNS:\n",
    "        -----------------\n",
    "        - Early days: More conservative, balanced strategies\n",
    "        - Later days: More aggressive, exploit remaining opportunities\n",
    "        - Last day: Maximum exploitation regardless of future consequences\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"TIME HORIZON EFFECTS ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for day in range(1, self.env.max_days + 1):\n",
    "            print(f\"\\nðŸ“… DAY {day} STRATEGY:\")\n",
    "            \n",
    "            # Find states for this day\n",
    "            day_states = [s for s in self.value_function.keys() if s.day == day]\n",
    "            \n",
    "            # Analyze action distribution\n",
    "            action_counts = {action: 0 for action in Action}\n",
    "            total_value = 0.0\n",
    "            \n",
    "            for state in day_states:\n",
    "                action = self.policy[state]\n",
    "                action_counts[action] += 1\n",
    "                total_value += self.value_function[state]\n",
    "            \n",
    "            total_states = len(day_states)\n",
    "            avg_value = total_value / total_states if total_states > 0 else 0\n",
    "            \n",
    "            print(f\"   Average State Value: ${avg_value:.2f}\")\n",
    "            print(f\"   Action Distribution ({total_states} states):\")\n",
    "            \n",
    "            for action, count in action_counts.items():\n",
    "                if count > 0:\n",
    "                    percentage = (count / total_states) * 100\n",
    "                    units_a, units_b = action.value\n",
    "                    print(f\"     {action.name:15} | {count:2d} states ({percentage:5.1f}%) | \"\n",
    "                          f\"Produces: {units_a}A, {units_b}B\")\n",
    "    \n",
    "    def analyze_value_function(self):\n",
    "        \"\"\"\n",
    "        VALUE FUNCTION ANALYSIS\n",
    "        ======================\n",
    "        \n",
    "        Examines the learned value function to identify:\n",
    "        - Most valuable states (best situations to be in)\n",
    "        - Least valuable states (situations to avoid)\n",
    "        - Value patterns across different state dimensions\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"VALUE FUNCTION ANALYSIS\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        # Find extreme value states\n",
    "        states_values = [(state, value) for state, value in self.value_function.items()]\n",
    "        states_values.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"\\nðŸ† TOP 10 MOST VALUABLE STATES:\")\n",
    "        for i, (state, value) in enumerate(states_values[:10]):\n",
    "            action = self.policy[state]\n",
    "            units_a, units_b = action.value\n",
    "            print(f\"   {i+1:2d}. Day {state.day}, RM={state.raw_material:2d}, \"\n",
    "                  f\"{state.market_state.name:15} | V=${value:6.2f} | \"\n",
    "                  f\"Action: {units_a}A,{units_b}B\")\n",
    "        \n",
    "        print(\"\\nðŸ“‰ BOTTOM 10 LEAST VALUABLE STATES:\")\n",
    "        for i, (state, value) in enumerate(states_values[-10:]):\n",
    "            action = self.policy[state]\n",
    "            units_a, units_b = action.value\n",
    "            print(f\"   {i+1:2d}. Day {state.day}, RM={state.raw_material:2d}, \"\n",
    "                  f\"{state.market_state.name:15} | V=${value:6.2f} | \"\n",
    "                  f\"Action: {units_a}A,{units_b}B\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        values = [v for _, v in states_values]\n",
    "        print(f\"\\nðŸ“ˆ VALUE FUNCTION STATISTICS:\")\n",
    "        print(f\"   Maximum Value: ${max(values):.2f}\")\n",
    "        print(f\"   Minimum Value: ${min(values):.2f}\")\n",
    "        print(f\"   Average Value: ${sum(values)/len(values):.2f}\")\n",
    "        \n",
    "        # Calculate standard deviation manually\n",
    "        mean_val = sum(values) / len(values)\n",
    "        variance = sum((x - mean_val) ** 2 for x in values) / len(values)\n",
    "        std_dev = variance ** 0.5\n",
    "        print(f\"   Standard Deviation: ${std_dev:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3305647e",
   "metadata": {},
   "source": [
    "#### --- 4. Impact of Dynamics Analysis --- (1 Mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2448380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discusses the impact of dynamic market prices on the optimal policy.\n",
    "# This section should primarily be a written explanation in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e6570fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ­ SMART SUPPLIER DYNAMIC PROGRAMMING SOLUTION\n",
      "================================================================================\n",
      "Optimizing production decisions under market uncertainty\n",
      "Using Value Iteration algorithm for exact optimal policy\n",
      "\n",
      "==================================================\n",
      "STEP 1: ENVIRONMENT SETUP\n",
      "==================================================\n",
      "SMART SUPPLIER ENVIRONMENT INITIALIZED\n",
      "==================================================\n",
      "Episode length: 5 days\n",
      "Daily raw materials: 10 units\n",
      "Product A cost: 2 RM per unit\n",
      "Product B cost: 1 RM per unit\n",
      "\n",
      "Market Pricing:\n",
      "  HIGH_DEMAND_A: A=$8, B=$2\n",
      "  HIGH_DEMAND_B: A=$3, B=$5\n",
      "\n",
      "==================================================\n",
      "STEP 2: DYNAMIC PROGRAMMING SOLUTION\n",
      "==================================================\n",
      "VALUE ITERATION ALGORITHM INITIALIZED\n",
      "========================================\n",
      "State space size: 110\n",
      "Discount factor: 1.0\n",
      "Convergence tolerance: 1e-06\n",
      "Maximum iterations: 1000\n",
      "\n",
      "============================================================\n",
      "STARTING VALUE ITERATION ALGORITHM\n",
      "============================================================\n",
      "\n",
      "Initializing value function...\n",
      "Initialized 110 states with V(s) = 0\n",
      "\n",
      "Iterating toward optimal value function...\n",
      "  Iteration   1: Max change = 25.00000000\n",
      "  Iteration   6: Max change = 0.00000000\n",
      "\n",
      "âœ“ CONVERGED after 6 iterations!\n",
      "  Final max change: 0.00e+00\n",
      "\n",
      "Extracting optimal policy...\n",
      "Extracted policy for 110 states\n",
      "\n",
      "ðŸŽ¯ VALUE ITERATION COMPLETE\n",
      "   Convergence achieved in 6 iterations\n",
      "\n",
      "==================================================\n",
      "STEP 3: OPTIMAL POLICY ANALYSIS\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "MARKET STATE DEPENDENCY ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“Š OPTIMAL STRATEGY IN HIGH_DEMAND_A:\n",
      "   Market Prices: A=$8, B=$2\n",
      "   Action Distribution (55 states):\n",
      "     PRODUCE_2A_0B   | 10 states ( 18.2%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  0 states (  0.0%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  0 states (  0.0%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   | 25 states ( 45.5%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      | 20 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "ðŸ“Š OPTIMAL STRATEGY IN HIGH_DEMAND_B:\n",
      "   Market Prices: A=$3, B=$5\n",
      "   Action Distribution (55 states):\n",
      "     PRODUCE_2A_0B   |  0 states (  0.0%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  5 states (  9.1%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   | 30 states ( 54.5%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  0 states (  0.0%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      | 20 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "======================================================================\n",
      "RESOURCE CONSTRAINT IMPACT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 0 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     DO_NOTHING      | 10 states (100.0%) | Cost: 0RM âœ“\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 2 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     DO_NOTHING      | 10 states (100.0%) | Cost: 0RM âœ“\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 4 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     PRODUCE_2A_0B   |  5 states ( 50.0%) | Cost: 4RM âœ“\n",
      "     PRODUCE_1A_2B   |  5 states ( 50.0%) | Cost: 4RM âœ“\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 6 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     PRODUCE_0A_5B   |  5 states ( 50.0%) | Cost: 5RM âœ“\n",
      "     PRODUCE_3A_0B   |  5 states ( 50.0%) | Cost: 6RM âœ“\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 8 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     PRODUCE_0A_5B   |  5 states ( 50.0%) | Cost: 5RM âœ“\n",
      "     PRODUCE_3A_0B   |  5 states ( 50.0%) | Cost: 6RM âœ“\n",
      "\n",
      "ðŸ”‹ STRATEGY WITH 10 RAW MATERIALS:\n",
      "   Action Distribution (10 states):\n",
      "     PRODUCE_0A_5B   |  5 states ( 50.0%) | Cost: 5RM âœ“\n",
      "     PRODUCE_3A_0B   |  5 states ( 50.0%) | Cost: 6RM âœ“\n",
      "\n",
      "======================================================================\n",
      "TIME HORIZON EFFECTS ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ“… DAY 1 STRATEGY:\n",
      "   Average State Value: $112.32\n",
      "   Action Distribution (22 states):\n",
      "     PRODUCE_2A_0B   |  2 states (  9.1%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  1 states (  4.5%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  6 states ( 27.3%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  5 states ( 22.7%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      |  8 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "ðŸ“… DAY 2 STRATEGY:\n",
      "   Average State Value: $87.82\n",
      "   Action Distribution (22 states):\n",
      "     PRODUCE_2A_0B   |  2 states (  9.1%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  1 states (  4.5%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  6 states ( 27.3%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  5 states ( 22.7%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      |  8 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "ðŸ“… DAY 3 STRATEGY:\n",
      "   Average State Value: $63.32\n",
      "   Action Distribution (22 states):\n",
      "     PRODUCE_2A_0B   |  2 states (  9.1%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  1 states (  4.5%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  6 states ( 27.3%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  5 states ( 22.7%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      |  8 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "ðŸ“… DAY 4 STRATEGY:\n",
      "   Average State Value: $38.82\n",
      "   Action Distribution (22 states):\n",
      "     PRODUCE_2A_0B   |  2 states (  9.1%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  1 states (  4.5%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  6 states ( 27.3%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  5 states ( 22.7%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      |  8 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "ðŸ“… DAY 5 STRATEGY:\n",
      "   Average State Value: $14.32\n",
      "   Action Distribution (22 states):\n",
      "     PRODUCE_2A_0B   |  2 states (  9.1%) | Produces: 2A, 0B\n",
      "     PRODUCE_1A_2B   |  1 states (  4.5%) | Produces: 1A, 2B\n",
      "     PRODUCE_0A_5B   |  6 states ( 27.3%) | Produces: 0A, 5B\n",
      "     PRODUCE_3A_0B   |  5 states ( 22.7%) | Produces: 3A, 0B\n",
      "     DO_NOTHING      |  8 states ( 36.4%) | Produces: 0A, 0B\n",
      "\n",
      "======================================================================\n",
      "VALUE FUNCTION ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "ðŸ† TOP 10 MOST VALUABLE STATES:\n",
      "    1. Day 1, RM= 5, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    2. Day 1, RM= 6, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    3. Day 1, RM= 7, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    4. Day 1, RM= 8, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    5. Day 1, RM= 9, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    6. Day 1, RM=10, HIGH_DEMAND_B   | V=$123.00 | Action: 0A,5B\n",
      "    7. Day 1, RM= 6, HIGH_DEMAND_A   | V=$122.00 | Action: 3A,0B\n",
      "    8. Day 1, RM= 7, HIGH_DEMAND_A   | V=$122.00 | Action: 3A,0B\n",
      "    9. Day 1, RM= 8, HIGH_DEMAND_A   | V=$122.00 | Action: 3A,0B\n",
      "   10. Day 1, RM= 9, HIGH_DEMAND_A   | V=$122.00 | Action: 3A,0B\n",
      "\n",
      "ðŸ“‰ BOTTOM 10 LEAST VALUABLE STATES:\n",
      "    1. Day 5, RM= 5, HIGH_DEMAND_A   | V=$ 16.00 | Action: 2A,0B\n",
      "    2. Day 5, RM= 4, HIGH_DEMAND_B   | V=$ 13.00 | Action: 1A,2B\n",
      "    3. Day 5, RM= 0, HIGH_DEMAND_A   | V=$  0.00 | Action: 0A,0B\n",
      "    4. Day 5, RM= 0, HIGH_DEMAND_B   | V=$  0.00 | Action: 0A,0B\n",
      "    5. Day 5, RM= 1, HIGH_DEMAND_A   | V=$  0.00 | Action: 0A,0B\n",
      "    6. Day 5, RM= 1, HIGH_DEMAND_B   | V=$  0.00 | Action: 0A,0B\n",
      "    7. Day 5, RM= 2, HIGH_DEMAND_A   | V=$  0.00 | Action: 0A,0B\n",
      "    8. Day 5, RM= 2, HIGH_DEMAND_B   | V=$  0.00 | Action: 0A,0B\n",
      "    9. Day 5, RM= 3, HIGH_DEMAND_A   | V=$  0.00 | Action: 0A,0B\n",
      "   10. Day 5, RM= 3, HIGH_DEMAND_B   | V=$  0.00 | Action: 0A,0B\n",
      "\n",
      "ðŸ“ˆ VALUE FUNCTION STATISTICS:\n",
      "   Maximum Value: $123.00\n",
      "   Minimum Value: $0.00\n",
      "   Average Value: $63.32\n",
      "   Standard Deviation: $36.44\n",
      "\n",
      "==================================================\n",
      "STEP 4: PERFORMANCE EVALUATION\n",
      "==================================================\n",
      "\n",
      "======================================================================\n",
      "POLICY SIMULATION: 1000 EPISODES\n",
      "======================================================================\n",
      "  Episode  100: Running average = $122.39\n",
      "  Episode  200: Running average = $122.57\n",
      "  Episode  300: Running average = $122.57\n",
      "  Episode  400: Running average = $122.58\n",
      "  Episode  500: Running average = $122.59\n",
      "  Episode  600: Running average = $122.54\n",
      "  Episode  700: Running average = $122.55\n",
      "  Episode  800: Running average = $122.53\n",
      "  Episode  900: Running average = $122.54\n",
      "  Episode 1000: Running average = $122.53\n",
      "\n",
      "ðŸ“Š SIMULATION RESULTS SUMMARY:\n",
      "   Episodes Simulated: 1000\n",
      "   Average Profit: $122.53 Â± $1.11\n",
      "   95% Confidence Interval: $122.46 - $122.60\n",
      "   Profit Range: $120.00 - $125.00\n",
      "\n",
      "ðŸŽ¯ ACTION USAGE DISTRIBUTION:\n",
      "   PRODUCE_2A_0B   |   0.0% | Produces: 2A, 0B\n",
      "   PRODUCE_1A_2B   |   0.0% | Produces: 1A, 2B\n",
      "   PRODUCE_0A_5B   |  50.6% | Produces: 0A, 5B\n",
      "   PRODUCE_3A_0B   |  49.4% | Produces: 3A, 0B\n",
      "   DO_NOTHING      |   0.0% | Produces: 0A, 0B\n",
      "\n",
      "==================================================\n",
      "STEP 5: COMPARATIVE ANALYSIS\n",
      "==================================================\n",
      "\n",
      "================================================================================\n",
      "DYNAMIC vs STATIC POLICY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "1. SOLVING DYNAMIC ENVIRONMENT (changing market conditions)...\n",
      "SMART SUPPLIER ENVIRONMENT INITIALIZED\n",
      "==================================================\n",
      "Episode length: 5 days\n",
      "Daily raw materials: 10 units\n",
      "Product A cost: 2 RM per unit\n",
      "Product B cost: 1 RM per unit\n",
      "\n",
      "Market Pricing:\n",
      "  HIGH_DEMAND_A: A=$8, B=$2\n",
      "  HIGH_DEMAND_B: A=$3, B=$5\n",
      "VALUE ITERATION ALGORITHM INITIALIZED\n",
      "========================================\n",
      "State space size: 110\n",
      "Discount factor: 1.0\n",
      "Convergence tolerance: 1e-06\n",
      "Maximum iterations: 1000\n",
      "\n",
      "============================================================\n",
      "STARTING VALUE ITERATION ALGORITHM\n",
      "============================================================\n",
      "\n",
      "Initializing value function...\n",
      "Initialized 110 states with V(s) = 0\n",
      "\n",
      "Iterating toward optimal value function...\n",
      "  Iteration   1: Max change = 25.00000000\n",
      "  Iteration   6: Max change = 0.00000000\n",
      "\n",
      "âœ“ CONVERGED after 6 iterations!\n",
      "  Final max change: 0.00e+00\n",
      "\n",
      "Extracting optimal policy...\n",
      "Extracted policy for 110 states\n",
      "\n",
      "ðŸŽ¯ VALUE ITERATION COMPLETE\n",
      "   Convergence achieved in 6 iterations\n",
      "\n",
      "2. SOLVING STATIC ENVIRONMENT A (always HIGH_DEMAND_A)...\n",
      "\n",
      "3. PERFORMANCE COMPARISON:\n",
      "   (Note: Full implementation would require separate static environments)\n",
      "   Dynamic policy adapts to market changes\n",
      "   Static policies would be suboptimal when market conditions change\n",
      "\n",
      "======================================================================\n",
      "POLICY SIMULATION: 1000 EPISODES\n",
      "======================================================================\n",
      "  Episode  100: Running average = $122.58\n",
      "  Episode  200: Running average = $122.52\n",
      "  Episode  300: Running average = $122.54\n",
      "  Episode  400: Running average = $122.56\n",
      "  Episode  500: Running average = $122.53\n",
      "  Episode  600: Running average = $122.51\n",
      "  Episode  700: Running average = $122.51\n",
      "  Episode  800: Running average = $122.48\n",
      "  Episode  900: Running average = $122.48\n",
      "  Episode 1000: Running average = $122.49\n",
      "\n",
      "ðŸ“Š SIMULATION RESULTS SUMMARY:\n",
      "   Episodes Simulated: 1000\n",
      "   Average Profit: $122.49 Â± $1.13\n",
      "   95% Confidence Interval: $122.42 - $122.56\n",
      "   Profit Range: $120.00 - $125.00\n",
      "\n",
      "ðŸŽ¯ ACTION USAGE DISTRIBUTION:\n",
      "   PRODUCE_2A_0B   |   0.0% | Produces: 2A, 0B\n",
      "   PRODUCE_1A_2B   |   0.0% | Produces: 1A, 2B\n",
      "   PRODUCE_0A_5B   |  49.7% | Produces: 0A, 5B\n",
      "   PRODUCE_3A_0B   |  50.3% | Produces: 3A, 0B\n",
      "   DO_NOTHING      |   0.0% | Produces: 0A, 0B\n",
      "\n",
      "ðŸ† DYNAMIC POLICY PERFORMANCE:\n",
      "   Average Profit: $122.49\n",
      "   Adapts to changing market conditions\n",
      "   Optimal for uncertain environment\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ¯ DYNAMIC PROGRAMMING SOLUTION COMPLETE\n",
      "================================================================================\n",
      "âœ“ Optimal policy learned using Value Iteration\n",
      "âœ“ Policy adapts to market conditions, resource constraints, and time horizon\n",
      "âœ“ Performance validated through Monte Carlo simulation\n",
      "âœ“ Strategic insights extracted through comprehensive analysis\n",
      "âœ“ Expected profit: $122.53 per 5-day period\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "\n",
    "def compare_policies():\n",
    "    \"\"\"\n",
    "    DYNAMIC vs STATIC POLICY COMPARISON\n",
    "    ==================================\n",
    "    \n",
    "    Compares the optimal policy learned in the dynamic environment\n",
    "    (with changing market conditions) to what would be optimal if\n",
    "    market conditions were always fixed.\n",
    "    \n",
    "    COMPARISON SCENARIOS:\n",
    "    ====================\n",
    "    1. DYNAMIC POLICY: Learned with market state transitions\n",
    "    2. STATIC POLICY A: Assumes market is always HIGH_DEMAND_A\n",
    "    3. STATIC POLICY B: Assumes market is always HIGH_DEMAND_B\n",
    "    \n",
    "    INSIGHTS:\n",
    "    ========\n",
    "    - How much value does adaptation provide?\n",
    "    - What strategies work best under uncertainty?\n",
    "    - How does optimal behavior change with and without market volatility?\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"DYNAMIC vs STATIC POLICY COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create environments for comparison\n",
    "    print(\"\\n1. SOLVING DYNAMIC ENVIRONMENT (changing market conditions)...\")\n",
    "    dynamic_env = SmartSupplierEnvironment()\n",
    "    dynamic_solver = ValueIteration(dynamic_env)\n",
    "    dynamic_values, dynamic_policy = dynamic_solver.solve()\n",
    "    \n",
    "    print(\"\\n2. SOLVING STATIC ENVIRONMENT A (always HIGH_DEMAND_A)...\")\n",
    "    # For static environment, we'd need to modify the environment\n",
    "    # This is a conceptual comparison - implementation would require\n",
    "    # environment modification\n",
    "    \n",
    "    print(\"\\n3. PERFORMANCE COMPARISON:\")\n",
    "    print(\"   (Note: Full implementation would require separate static environments)\")\n",
    "    print(\"   Dynamic policy adapts to market changes\")\n",
    "    print(\"   Static policies would be suboptimal when market conditions change\")\n",
    "    \n",
    "    # Simulate dynamic policy\n",
    "    simulator = PolicySimulator(dynamic_env, dynamic_policy)\n",
    "    dynamic_results = simulator.run_simulation(1000)\n",
    "    \n",
    "    print(f\"\\nðŸ† DYNAMIC POLICY PERFORMANCE:\")\n",
    "    print(f\"   Average Profit: ${dynamic_results['mean_reward']:.2f}\")\n",
    "    print(f\"   Adapts to changing market conditions\")\n",
    "    print(f\"   Optimal for uncertain environment\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    MAIN EXECUTION FUNCTION\n",
    "    ======================\n",
    "    \n",
    "    Orchestrates the complete Dynamic Programming solution for the\n",
    "    Smart Supplier problem, including:\n",
    "    \n",
    "    1. Environment setup\n",
    "    2. Value Iteration algorithm execution\n",
    "    3. Policy analysis and interpretation\n",
    "    4. Performance evaluation through simulation\n",
    "    5. Comparative analysis\n",
    "    \n",
    "    This provides a comprehensive demonstration of Dynamic Programming\n",
    "    applied to real-world optimization problems.\n",
    "    \"\"\"\n",
    "    print(\"ðŸ­ SMART SUPPLIER DYNAMIC PROGRAMMING SOLUTION\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Optimizing production decisions under market uncertainty\")\n",
    "    print(\"Using Value Iteration algorithm for exact optimal policy\")\n",
    "    \n",
    "    # STEP 1: ENVIRONMENT SETUP\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 1: ENVIRONMENT SETUP\")\n",
    "    print(\"=\"*50)\n",
    "    env = SmartSupplierEnvironment()\n",
    "    \n",
    "    # STEP 2: SOLVE USING VALUE ITERATION\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 2: DYNAMIC PROGRAMMING SOLUTION\")\n",
    "    print(\"=\"*50)\n",
    "    solver = ValueIteration(env)\n",
    "    value_function, optimal_policy = solver.solve()\n",
    "    \n",
    "    # STEP 3: POLICY ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 3: OPTIMAL POLICY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    analyzer = PolicyAnalyzer(env, value_function, optimal_policy)\n",
    "    analyzer.analyze_market_dependency()\n",
    "    analyzer.analyze_resource_dependency()\n",
    "    analyzer.analyze_time_dependency()\n",
    "    analyzer.analyze_value_function()\n",
    "    \n",
    "    # STEP 4: PERFORMANCE EVALUATION\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 4: PERFORMANCE EVALUATION\")\n",
    "    print(\"=\"*50)\n",
    "    simulator = PolicySimulator(env, optimal_policy)\n",
    "    simulation_results = simulator.run_simulation(1000)\n",
    "    \n",
    "    # STEP 5: COMPARATIVE ANALYSIS\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"STEP 5: COMPARATIVE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    compare_policies()\n",
    "    \n",
    "    # FINAL SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸŽ¯ DYNAMIC PROGRAMMING SOLUTION COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ“ Optimal policy learned using Value Iteration\")\n",
    "    print(\"âœ“ Policy adapts to market conditions, resource constraints, and time horizon\")\n",
    "    print(\"âœ“ Performance validated through Monte Carlo simulation\")\n",
    "    print(\"âœ“ Strategic insights extracted through comprehensive analysis\")\n",
    "    print(f\"âœ“ Expected profit: ${simulation_results['mean_reward']:.2f} per 5-day period\")\n",
    "    \n",
    "    return {\n",
    "        'environment': env,\n",
    "        'value_function': value_function,\n",
    "        'optimal_policy': optimal_policy,\n",
    "        'simulation_results': simulation_results\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Execute the complete Dynamic Programming solution\n",
    "    results = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
